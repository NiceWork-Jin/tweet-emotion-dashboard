{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ca22b78",
   "metadata": {},
   "source": [
    "# Execution order\n",
    "1. Turn on main.py in tweepy2Spark\n",
    "2. Turn on the SocketListener in the same folder.\n",
    "3. Run all in this file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000529a9",
   "metadata": {},
   "source": [
    "### Create SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84f15028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Predicting TweetWorld Emotion\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e68fd4",
   "metadata": {},
   "source": [
    "### UDF (Transform from JSON Str with Sigle quotes to Double quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ad5c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast \n",
    "import json \n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def convert_json_double(json_single):\n",
    "    json_dict = ast.literal_eval(json_single)\n",
    "    return json.dumps(json_dict)\n",
    "    \n",
    "convert_json_double_udf = udf(lambda x: convert_json_double(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5920b7f4",
   "metadata": {},
   "source": [
    "### Get the column names from sample tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c69c749a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "with open(\"data/tweet.txt\", 'r') as f:\n",
    "    originSingleQuotes = f.readline()\n",
    "    originDoubleQuotes = convert_json_double(originSingleQuotes)\n",
    "\n",
    "sc = spark.sparkContext\n",
    "originRDD = sc.parallelize([originDoubleQuotes])\n",
    "originDF = spark.read.json(originRDD)\n",
    "\n",
    "columns = originDF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49333577",
   "metadata": {},
   "source": [
    "### Send a connection request to Server Socket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3df11ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "socketDF = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1bc4b5",
   "metadata": {},
   "source": [
    "### Transfrom data to analyze and extract data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b18825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import json_tuple\n",
    "\n",
    "jsonDF = socketDF.select(convert_json_double_udf(\"value\").alias(\"value\"))\n",
    "multiColDF = jsonDF.select(json_tuple(\"value\", *columns)).toDF(*columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a0a627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = multiColDF.select(\"created_at\", \"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f69ea35",
   "metadata": {},
   "source": [
    "### Filter in English before estimating emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ff62bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def detect_language(text):\n",
    "    return detect(text)\n",
    "\n",
    "detect_language_udf = F.udf(lambda x: detect_language(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c661d4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(\"created_at\",\"text\", detect_language_udf(\"text\").alias(\"lang\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c973386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df = df.filter(col(\"lang\") == \"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4e51e5",
   "metadata": {},
   "source": [
    "### Estimating emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5631ac5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "positive = 2\n",
    "netural = 1\n",
    "negotive = 0\n",
    "\n",
    "def get_sentiment(text):\n",
    "    sentiment = TextBlob(text).sentiment.polarity\n",
    "    if sentiment > 0:\n",
    "        return positive\n",
    "    elif sentiment < 0:\n",
    "        return negotive\n",
    "    else:\n",
    "        return netural\n",
    "\n",
    "get_sentiment_utf = F.udf(lambda x: get_sentiment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61fea887",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentDF = df.select(\"created_at\", \"text\", get_sentiment_utf(col(\"text\")).alias(\"sentiment_level\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295d2c6c",
   "metadata": {},
   "source": [
    "### Aggregate sentiment_level by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8e530e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[time_interval: timestamp, sentiment_level: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert created_at's type to datetime \n",
    "import datetime\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "def from_created_at(x):\n",
    "    \"\"\"\n",
    "    parsing format : \"https://docs.python.org/3/library/datetime.html#datetime.date\"\n",
    "    \n",
    "    The valuable of 'x' has a form of 'Thu Oct 21 07:02:44 +0000 2021' \n",
    "    \"\"\"\n",
    "    dt = datetime.datetime.strptime(x, \"%a %b %d %H:%M:%S %z %Y\")\n",
    "    return dt.isoformat()\n",
    "\n",
    "from_created_at_udf = udf(lambda x: from_created_at(x))\n",
    "\n",
    "df = sentimentDF.select( \n",
    "    from_created_at_udf(col(\"created_at\")).cast(TimestampType()).alias(\"time_interval\"), col(\"sentiment_level\")\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89e87d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "windowedBySec = df \\\n",
    "    .withWatermark(\"time_interval\", \"0 seconds\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"time_interval\"), \"1 seconds\"), col(\"sentiment_level\")\n",
    "    ) \\\n",
    "    .count()\n",
    "\n",
    "windowedByMin = df \\\n",
    "    .withWatermark(\"time_interval\", \"0 seconds\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"time_interval\"), \"1 minutes\"), col(\"sentiment_level\")\n",
    "    ) \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a7f85e",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d3482b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "host = '127.0.0.1'\n",
    "port = 5000\n",
    "uri = 'update_react_num_per_sec'\n",
    "new_name = [\"positive_num\", \"neutral_num\", \"negative_num\"]\n",
    "\n",
    "def foreach_batch_json(df, epoch_id):\n",
    "    count_by_level = df \\\n",
    "        .groupBy(\"window\") \\\n",
    "        .pivot(\"sentiment_level\", [\"0\", \"1\", \"2\"]) \\\n",
    "        .sum(\"count\") \\\n",
    "        .drop(\"window\") \\\n",
    "        .na.fill(0) \\\n",
    "        .toDF(*new_name)\n",
    "    \n",
    "    data_list = count_by_level \\\n",
    "        .toJSON() \\\n",
    "        .collect()\n",
    "    \n",
    "    if not data_list:\n",
    "        return\n",
    "\n",
    "    for data in data_list:\n",
    "        requests.post(f'http://{host}:{port}/{uri}', data=json.loads(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b874d0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ExecSec = windowedBySec \\\n",
    "    .writeStream \\\n",
    "    .trigger(once=True) \\\n",
    "    .foreachBatch(foreach_batch_json) \\\n",
    "    .start()\n",
    "\n",
    "# ExecMin = windowedByMin \\\n",
    "#     .writeStream \\\n",
    "#     .foreachBatch(foreach_batch_json) \\\n",
    "#     .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bb35f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ExecSec.stop() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
