{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0191c849",
   "metadata": {},
   "source": [
    "## SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f15028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Predicting TweetWorld Emotion\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# TEST\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e68fd4",
   "metadata": {},
   "source": [
    "## UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad5c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast \n",
    "import json \n",
    "from langdetect import detect\n",
    "from textblob import TextBlob\n",
    "import datetime\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "def convert_json_double(json_single):\n",
    "    json_dict = ast.literal_eval(json_single)\n",
    "    return json.dumps(json_dict)\n",
    "    \n",
    "def detect_language(text):\n",
    "    return detect(text)\n",
    "\n",
    "def get_sentiment(text):\n",
    "    sentiment = TextBlob(text).sentiment.polarity\n",
    "    if sentiment > 0:\n",
    "        return 1    # Positive\n",
    "    elif sentiment < 0:\n",
    "        return -1    # Negative\n",
    "    else:\n",
    "        return 0    # Neutral\n",
    "\n",
    "def from_created_at(x):\n",
    "    \"\"\"\n",
    "    parsing format : \"https://docs.python.org/3/library/datetime.html#datetime.date\"\n",
    "    \n",
    "    The valuable of 'x' has a form of 'Thu Oct 21 07:02:44 +0000 2021' \n",
    "    \"\"\"\n",
    "    dt = datetime.datetime.strptime(x, \"%a %b %d %H:%M:%S %z %Y\")\n",
    "    return dt.isoformat()\n",
    "\n",
    "convert_json_double_udf = udf(lambda x: convert_json_double(x))\n",
    "detect_language_udf = udf(lambda x: detect_language(x))\n",
    "get_sentiment_utf = udf(lambda x: get_sentiment(x))\n",
    "from_created_at_udf = udf(lambda x: from_created_at(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5920b7f4",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69c749a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import json_tuple\n",
    "\n",
    "# Use sample data to easily extract column names through inferSchema\n",
    "with open(\"data/tweet.txt\", 'r') as f:\n",
    "    SingleQuotesJSON = f.readline()\n",
    "    DoubleQuotesJSON = convert_json_double(SingleQuotesJSON)\n",
    "\n",
    "sampleRDD = spark.sparkContext.parallelize([DoubleQuotesJSON])\n",
    "sampleDF = spark.read.json(sampleRDD)\n",
    "columns = sampleDF.columns\n",
    "\n",
    "# Read Streaming data from socket\n",
    "socketDF = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# Load only neccesary data\n",
    "originDF = socketDF.select(convert_json_double_udf(\"value\").alias(\"value\")) \\\n",
    "    .select(json_tuple(\"value\", *columns)).toDF(*columns) \\\n",
    "    .select(\"created_at\", \"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234a98a6",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c661d4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "readyDF = originDF.select(\n",
    "    # Convert the form of \"created_at\" to ISO format for casting as TimestampType.\n",
    "    from_created_at_udf(col(\"created_at\")).cast(TimestampType()).alias(\"created_at\"),\n",
    "    \n",
    "    # Estimate sentiment level. Positive, Neutral and Negative.\n",
    "    get_sentiment_utf(col(\"text\")).alias(\"sentiment_level\"),\n",
    "    \n",
    "    # Verify langage through text.\n",
    "    detect_language_udf(col(\"text\")).alias(\"lang\"),\n",
    "    \n",
    "    # Extract a hashtag from the text. It starts with \"@\"\n",
    "    regexp_extract(col(\"text\"), '(@\\w+)', 1).alias(\"hashtag\")\n",
    ").filter(col(\"lang\") == \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff779dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "readyDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b45b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "file_name = \"ready_table\"\n",
    "\n",
    "spark_warehouse_loc = f'./spark-warehouse/{file_name}'\n",
    "checkpoint_loc = f'./checkpoint/dir/{file_name}'\n",
    "\n",
    "if os.path.exists(spark_warehouse_loc):\n",
    "    shutil.rmtree(spark_warehouse_loc)\n",
    "    \n",
    "if os.path.exists(checkpoint_loc):\n",
    "    shutil.rmtree(checkpoint_loc)\n",
    "    \n",
    "readyDF.writeStream \\\n",
    "    .option(\"checkpointLocation\", checkpoint_loc) \\\n",
    "    .toTable(file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4286e56b",
   "metadata": {},
   "source": [
    "# Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d72dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, rank, sum\n",
    "from pyspark.sql.window import Window\n",
    "import requests\n",
    "import json\n",
    "import ast\n",
    "\n",
    "class WindowAggregator(object):\n",
    "    def __init__(self, uri, host='127.0.0.1', port=5000):\n",
    "        self.uri = uri\n",
    "        self.target = f'http://{host}:{port}/{uri}'\n",
    "        \n",
    "    def sentiment_level_num(self, df, epoch_id):\n",
    "        countByLevelDF = df \\\n",
    "            .groupBy(\"window\") \\\n",
    "            .pivot(\"sentiment_level\", ['1', '0', '-1']) \\\n",
    "            .sum(\"count\") \\\n",
    "            .drop(\"window\") \\\n",
    "            .na.fill(0) \\\n",
    "            .toDF('positive', 'neutral', 'negative')\n",
    "\n",
    "        self.send_data(countByLevelDF)\n",
    "    \n",
    "    def hashtag_top_five(self, df, epoch_id):\n",
    "        window = Window \\\n",
    "            .partitionBy(df['window']) \\\n",
    "            .orderBy(df['count'].desc(), df['hashtag'])\n",
    "        \n",
    "        rank4SortDF = df \\\n",
    "            .select('*', rank() \\\n",
    "            .over(window).alias('rank')) \\\n",
    "            .filter(col('rank') <= 2) \n",
    "        \n",
    "        hashTagTopFiveDF = rank4SortDF \\\n",
    "            .groupBy(\"window\") \\\n",
    "            .pivot(\"hashtag\") \\\n",
    "            .agg(sum(\"count\")) \\\n",
    "            .drop(\"window\")\n",
    "        \n",
    "        self.send_data_with_nested_form(hashTagTopFiveDF)\n",
    "                   \n",
    "            \n",
    "    def send_data(self, df):\n",
    "        data_list = df \\\n",
    "            .toJSON() \\\n",
    "            .collect()\n",
    "\n",
    "        if not data_list:\n",
    "            return\n",
    "\n",
    "        for data in data_list:\n",
    "            requests.post(\n",
    "                self.target,\n",
    "                data=json.loads(data)\n",
    "            )\n",
    "        \n",
    "        \n",
    "    def send_data_with_nested_form(self, df):\n",
    "        data_list = df \\\n",
    "            .toJSON() \\\n",
    "            .collect()\n",
    "\n",
    "        if not data_list:\n",
    "            return\n",
    "        \n",
    "        for data in data_list:\n",
    "            form = {'data': ''}\n",
    "            form_inner = json.dumps(ast.literal_eval(data))\n",
    "            form['data'] = form_inner\n",
    "            \n",
    "            requests.post(\n",
    "                self.target,\n",
    "                data = form\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6420061e",
   "metadata": {},
   "source": [
    "## Sentiment_level_Sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fd7b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentLevelNumSecDF = spark.readStream \\\n",
    "    .table(\"ready_table\") \\\n",
    "    .withWatermark(\"created_at\", \"2 seconds\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"created_at\"), \"1 seconds\"), col(\"sentiment_level\")\n",
    "    ) \\\n",
    "    .count()\n",
    "\n",
    "exec_sentimentLevelNumSecDF = sentimentLevelNumSecDF \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(\n",
    "        WindowAggregator('update/sentiment_level_number/sec/1') \\\n",
    "        .sentiment_level_num) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8b4770",
   "metadata": {},
   "source": [
    "## Sentiment_level_Min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bb4e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentLevelNumMinDF = spark.readStream \\\n",
    "    .table(\"ready_table\") \\\n",
    "    .withWatermark(\"created_at\", \"1 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"created_at\"), \"20 seconds\", \"10 seconds\"), col(\"sentiment_level\")\n",
    "    ) \\\n",
    "    .count()\n",
    "\n",
    "exec_sentimentLevelNumSecDF = sentimentLevelNumMinDF \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(\n",
    "        WindowAggregator('update/sentiment_level_number/min/1') \\\n",
    "        .sentiment_level_num) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79d971d",
   "metadata": {},
   "source": [
    "## Top5 HashTab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed310a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "HashTagNumMinDF = spark.readStream \\\n",
    "    .table(\"ready_table\") \\\n",
    "    .withWatermark(\"created_at\", \"1 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"created_at\"), \"20 seconds\", \"10 seconds\"), col(\"hashtag\")\n",
    "    ) \\\n",
    "    .count() \\\n",
    "    .where(col(\"hashtag\") != '')\n",
    "\n",
    "exec_sentimentLevelNumSecDF = HashTagNumMinDF \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(\n",
    "        WindowAggregator('update/top_five_hashtags/min/1') \\\n",
    "        .hashtag_top_five) \\\n",
    "    .start() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d20bad",
   "metadata": {},
   "source": [
    "## sentimentScore_TimeSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3896906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.readStream \\\n",
    "#     .table(\"ready_table\") \\\n",
    "#     .withWatermark(\"created_at\", \"1 seconds\") \\\n",
    "#     .groupBy(\n",
    "#         window(col(\"created_at\"), \"20 seconds\"), col(\"hashtag\")\n",
    "#     ) \\\n",
    "#     .count() \\\n",
    "#     .where(col(\"hashtag\") != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fe27cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.streams.awaitAnyTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
