{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "000529a9",
   "metadata": {},
   "source": [
    "# Create SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "84f15028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Predicting TweetWorld Emotion\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e68fd4",
   "metadata": {},
   "source": [
    "# UDF (Transform from JSON Str with Sigle quotes to Double quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "9ad5c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast \n",
    "import json \n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def convert_json_double(json_single):\n",
    "    json_dict = ast.literal_eval(json_single)\n",
    "    return json.dumps(json_dict)\n",
    "    \n",
    "convert_json_double_udf = udf(lambda x: convert_json_double(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5920b7f4",
   "metadata": {},
   "source": [
    "# Get the column names from sample tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "c69c749a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "with open(\"data/tweet.txt\", 'r') as f:\n",
    "    originSingleQuotes = f.readline()\n",
    "    originDoubleQuotes = convert_json_double(originSingleQuotes)\n",
    "\n",
    "sc = spark.sparkContext\n",
    "originRDD = sc.parallelize([originDoubleQuotes])\n",
    "originDF = spark.read.json(originRDD)\n",
    "\n",
    "columns = originDF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49333577",
   "metadata": {},
   "source": [
    "# Send a connection request to Server Socket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f3df11ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "socketDF = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1bc4b5",
   "metadata": {},
   "source": [
    "# Transfrom data to analyze and extract data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "8b18825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import json_tuple\n",
    "\n",
    "jsonDF = socketDF.select(convert_json_double_udf(\"value\").alias(\"value\"))\n",
    "multiColDF = jsonDF.select(json_tuple(\"value\", *columns)).toDF(*columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "1a0a627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = multiColDF.select(\"created_at\", \"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f69ea35",
   "metadata": {},
   "source": [
    "# Filter in English before estimating emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2ff62bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def detect_language(text):\n",
    "    return detect(text)\n",
    "\n",
    "detect_language_udf = F.udf(lambda x: detect_language(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c661d4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(\"created_at\",\"text\", detect_language_udf(\"text\").alias(\"lang\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "7c973386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df = df.filter(col(\"lang\") == \"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4e51e5",
   "metadata": {},
   "source": [
    "# Estimating emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "5631ac5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "positive = 2\n",
    "netural = 1\n",
    "negotive = 0\n",
    "\n",
    "def get_sentiment(text):\n",
    "    sentiment = TextBlob(text).sentiment.polarity\n",
    "    if sentiment > 0:\n",
    "        return positive\n",
    "    elif sentiment < 0:\n",
    "        return negotive\n",
    "    else:\n",
    "        return netural\n",
    "\n",
    "get_sentiment_utf = F.udf(lambda x: get_sentiment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "61fea887",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentDF = df.select(\"created_at\", \"text\", get_sentiment_utf(col(\"text\")).alias(\"sentiment_level\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295d2c6c",
   "metadata": {},
   "source": [
    "# Aggregate sentiment_level by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c385c321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e8e530e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_datetime(txt):\n",
    "    month_to_m = {\n",
    "        'Jan': 1,\n",
    "        'Feb': 2,\n",
    "        'Mar': 3,\n",
    "        'Apr': 4,\n",
    "        'May': 5,\n",
    "        'Jun': 6,\n",
    "        'Jul': 7,\n",
    "        'Aug': 8,\n",
    "        'Sep': 9,\n",
    "        'Oct': 10,\n",
    "        'Nov': 11, \n",
    "        'Dec': 12,       \n",
    "    }\n",
    "    \n",
    "#     txt = 'Thu Oct 21 07:02:44 +0000 2021'\n",
    "    tmp = txt.split(\" \")\n",
    "    year = tmp[5]\n",
    "    day_of_the_week = tmp[0]\n",
    "    month = month_to_m[tmp[1]]\n",
    "    day = tmp[2]\n",
    "    time = tmp[3]\n",
    "    return f'{month}-{day}-{year}'\n",
    "\n",
    "text_datetime_udf = udf(lambda x: text_datetime(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "a64e4f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = sentimentDF.select(text_datetime_udf(col(\"created_at\")).alias(\"created_at\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "118d4164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[created_at: string]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f04ce95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.select(\"created_at\", to_date(col(\"created_at\"), 'MM-dd-yyyy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ecf33c",
   "metadata": {},
   "source": [
    "# run and debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "6fb87a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "launch = new_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .queryName(\"sentimentDF\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "6b79671b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------------------+\n",
      "|created_at|to_date(created_at, MM-dd-yyyy)|\n",
      "+----------+-------------------------------+\n",
      "|10-27-2021|                     2021-10-27|\n",
      "|10-27-2021|                     2021-10-27|\n",
      "|10-27-2021|                     2021-10-27|\n",
      "+----------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from sentimentDF\").show(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "41c3bc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
